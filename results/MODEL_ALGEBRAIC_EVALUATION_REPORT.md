# Model-Algebraic Generalized PCA: Comprehensive Evaluation Report\n\nThis report evaluates the **model-algebraic framework** from Chapter 4, where we keep data structure X̃ = [y, X] fixed and vary operator constraints to represent different statistical models.\n\n## Executive Summary\n\n**Datasets Evaluated**: 3\n\n### Theoretical Framework Validation\n\n- **Constraint Satisfaction**: 100.0% (Regression A_μ = 0, PCA A_β = 0)\n- **sklearn Equivalence**: 100.0% (Constrained operators match sklearn)\n- **Overall Success Rate**: 100.0%\n\n### Performance Analysis\n\n- **Model-Algebraic Regression vs sklearn**: 1.000x MSE ratio\n- **Joint Model vs sklearn**: 87435.438x MSE ratio\n- **Joint vs Pure Regression**: 87435.438x MSE ratio\n\n### Theoretical Insights\n\n- **Implicit Weighting Evidence**: 66.7% of datasets show joint model trading off prediction for reconstruction\n\n## Detailed Dataset Results\n\n### synthetic_regression\n\n- **Samples**: 160 train, 40 test\n- **Features**: 5 (augmented dimension k = 6)\n\n#### Performance Summary\n\n- **sklearn Linear Regression**: 0.000001 MSE\n- **Model-Algebraic Regression**: 0.000001 MSE (r=1)\n- **Model-Algebraic Joint**: 0.245044 MSE (r=5)\n\n#### Theoretical Validation: ✅ PASSED\n\n- **Constraint Satisfaction**: True\n- **sklearn Equivalence**: True (diff: 5.81e-20)\n\n\n### diabetes\n\n- **Samples**: 353 train, 89 test\n- **Features**: 10 (augmented dimension k = 11)\n\n#### Performance Summary\n\n- **sklearn Linear Regression**: 0.477288 MSE\n- **Model-Algebraic Regression**: 0.477288 MSE (r=1)\n- **Model-Algebraic Joint**: 0.491506 MSE (r=4)\n\n#### Theoretical Validation: ✅ PASSED\n\n- **Constraint Satisfaction**: True\n- **sklearn Equivalence**: True (diff: 0.00e+00)\n\n\n### california_housing_subset\n\n- **Samples**: 800 train, 200 test\n- **Features**: 8 (augmented dimension k = 9)\n\n#### Performance Summary\n\n- **sklearn Linear Regression**: 0.391929 MSE\n- **Model-Algebraic Regression**: 0.391929 MSE (r=1)\n- **Model-Algebraic Joint**: 0.577382 MSE (r=5)\n\n#### Theoretical Validation: ✅ PASSED\n\n- **Constraint Satisfaction**: True\n- **sklearn Equivalence**: True (diff: 1.17e-15)\n\n\n## Theoretical Implications\n\n### 1. Framework Validation\n\nThe model-algebraic framework successfully demonstrates:\n\n- ✅ **Constraint enforcement**: Operator blocks are mathematically constrained as specified\n- ✅ **Method equivalence**: Constrained operators exactly reproduce sklearn methods\n- ✅ **Unified representation**: Different models emerge from single operator algebra\n\n### 2. Implicit Weighting Confirmation\n\nJoint model consistently shows implicit weighting behavior:\n\n- **Prediction performance** typically worse than pure regression\n- **Trade-off emerges naturally** from SVD optimization\n- **No manual parameter tuning** required\n\n### 3. Model-Algebraic Advantages\n\n- **Theoretical rigor**: Semiring structure provides mathematical foundation\n- **Principled comparison**: Unified framework enables fair model comparison\n- **Natural extensions**: Framework extends to other statistical methods\n- **Operator composition**: Models can be combined algebraically\n\n## Conclusion\n\nThe model-algebraic generalized PCA framework provides a **theoretically rigorous** and **empirically validated** approach to unifying regression and PCA. By treating statistical methods as operator constraints rather than separate algorithms, we gain both theoretical insights and practical tools for principled model development.\n\n**Key Achievement**: Demonstrated that the operator-choice problem from Chapter 4 can be successfully implemented and validated on real-world data, confirming the theoretical framework's practical value.\n