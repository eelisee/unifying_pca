\section{Outlook}

\paragraph{Conclusion.}
PCA regression illustrates how dimensionality reduction and prediction can be jointly modeled through structured linear operators. It fits naturally into our unified matrix-based framework, and serves as a concrete example where variance-preserving transformations and regression estimation coexist within the same algebraic structure.

\subsection{Future Work}
\begin{itemize}
    \item Define and use variation as in Schlather \& Reinbott.
    \item Explore orthogonality of selected variables via scalar products.
    \item Analyze algebraic closures and group-like extensions (e.g., quasi-inverses).
    \item Extending the coincidence conditions to heavy-tailed or stable distributions 
    where classical variance is undefined. 
    \item An algebraic classification of operator subsets (subsemirings, ideals) 
    and their statistical interpretation. 
    \item Simulation studies beyond the Gaussian case to illustrate 
    the persistence of contradictions. 
\end{itemize}

\subsection{Conclusion}
We provide a theoretical framework that explains why PCA and regression differ, despite acting on similar statistical data. The structure introduced here enables PCA to be seen as a constrained case of statistical estimation â€” specifically, one where directionality is relaxed. This algebraic treatment lays the foundation for generalized PCA and structured variable selection models.